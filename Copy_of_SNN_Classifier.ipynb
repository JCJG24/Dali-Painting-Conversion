{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JCJG24/Various-Projects/blob/PyTorch_Object_Detection/Copy_of_SNN_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c!pip install snntorch --quiet"
      ],
      "metadata": {
        "id": "l-EtxSz0CYP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import itertools\n",
        "import snntorch as snn\n",
        "import os\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "oqSVsOpo8JeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "5P9mcVlZdoBL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d89045a-d5ba-4688-a7c7-9078a8d27e14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.9/dist-packages (1.5.13)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.9/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.27.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.26.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.9/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (3.4)\n",
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#! kaggle datasets download mikoajfish99/lions-or-cheetahs-image-classification\n",
        "#! unzip lions-or-cheetahs-image-classification\n",
        "! kaggle competitions download -c dogs-vs-cats\n",
        "! unzip dogs-vs-cats"
      ],
      "metadata": {
        "id": "lwYzW4tWeGWx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bc7ee94-4851-4f09-a498-913767c19551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/kaggle/api/kaggle_api_extended.py\", line 164, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n",
            "unzip:  cannot find or open dogs-vs-cats, dogs-vs-cats.zip or dogs-vs-cats.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip test1\n",
        "! unzip train"
      ],
      "metadata": {
        "id": "Ia-_HGxjei53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataloader arguments\n",
        "\n",
        "batch_size = 128\n",
        "data_path_train = '/content/train'\n",
        "data_path_test = '/content/test1'\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "7oLBg8Ry845_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform definition\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0,), (1,))\n",
        "])\n",
        "\n",
        "#mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
        "#mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "cz9_9Qdo9pwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create smaller sample directory\n",
        "small_train_datapath = '/content/train_small'\n",
        "#small_test_datapath = '/content/test_small'\n",
        "os.mkdir(small_train_datapath)\n",
        "#os.mkdir(small_test_datapath)"
      ],
      "metadata": {
        "id": "CRzoYW26y5T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill the smaller sample directory\n",
        "import shutil\n",
        "fnames = ['cat.{}.jpg'.format(i) for i in range(10000)]\n",
        "for fname in fnames:\n",
        "  origin = os.path.join(data_path_train, fname)\n",
        "  destination = os.path.join(small_train_datapath, fname)\n",
        "  shutil.copyfile(origin, destination)\n",
        "\n",
        "fnames = ['dog.{}.jpg'.format(i) for i in range(10000)]\n",
        "for fname in fnames:\n",
        "  origin = os.path.join(data_path_train, fname)\n",
        "  destination = os.path.join(small_train_datapath, fname)\n",
        "  shutil.copyfile(origin, destination)"
      ],
      "metadata": {
        "id": "QBFssUwvzpIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "train_list = glob.glob(os.path.join(small_train_datapath,'*.jpg'))\n",
        "test_list = glob.glob(os.path.join(data_path_test, '*.jpg'))"
      ],
      "metadata": {
        "id": "0DDrmuPiqU1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_list), len(test_list))"
      ],
      "metadata": {
        "id": "FMh9ZSz-qkpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the Data + Imports\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "random_idx = np.random.randint(1,20000,size=10)\n",
        "\n",
        "fig = plt.figure()\n",
        "i=1\n",
        "for idx in random_idx:\n",
        "    ax = fig.add_subplot(2,5,i)\n",
        "    img = Image.open(train_list[idx])\n",
        "    plt.imshow(img)\n",
        "    i+=1\n",
        "\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1Pr0INoCrhVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_list[0].split('/')[-1].split('.')[0]"
      ],
      "metadata": {
        "id": "xZMfb2kUsUtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "int(test_list[0].split('/')[-1].split('.')[0])"
      ],
      "metadata": {
        "id": "zdf6ZYKBsdiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the validation list sampling from the train list\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_list, val_list = train_test_split(train_list, test_size=0.2)"
      ],
      "metadata": {
        "id": "V5b4YyCSsj-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(val_list))"
      ],
      "metadata": {
        "id": "ZoQimgy_s4wQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the custom Dataset\n",
        "class dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,file_list,transform=None):\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "        \n",
        "        \n",
        "    #dataset length\n",
        "    def __len__(self):\n",
        "        self.filelength = len(self.file_list)\n",
        "        return self.filelength\n",
        "    \n",
        "    #load an one of images\n",
        "    def __getitem__(self,idx):\n",
        "        img_path = self.file_list[idx]\n",
        "        img = Image.open(img_path)\n",
        "        img_transformed = self.transform(img)\n",
        "        \n",
        "        label = img_path.split('/')[-1].split('.')[0]\n",
        "        if label == 'dog':\n",
        "            label=1\n",
        "            #label = torch.tensor(1, dtype=torch.long)\n",
        "        elif label == 'cat':\n",
        "            label=0\n",
        "            #label = torch.tensor(0, dtype=torch.long)\n",
        "\n",
        "\n",
        "        return img_transformed,label"
      ],
      "metadata": {
        "id": "PKopT5T_wkB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the data for the Dataloaders\n",
        "train_data = dataset(train_list, transform=transform)\n",
        "test_data = dataset(test_list, transform=transform)\n",
        "val_data = dataset(val_list, transform=transform)"
      ],
      "metadata": {
        "id": "6e_alLL8wx5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "fKOtQbTu_RQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Network architecture\n",
        "num_inputs = 28*28\n",
        "num_hidden = 1000\n",
        "num_outputs = 2\n",
        "\n",
        "#Temporal Dynamics\n",
        "num_steps = 50\n",
        "beta = 0.95"
      ],
      "metadata": {
        "id": "mefoVvlABqJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one of many / simpler\n",
        "class ConvNet(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # initialize layers\n",
        "    self.conv1 = nn.Conv2d(1, 64, 3, padding=\"same\")\n",
        "    self.mxpl1 = nn.MaxPool2d(2)\n",
        "    self.lif1 = snn.Leaky(beta=beta)\n",
        "\n",
        "    self.conv2 = nn.Conv2d(64, 128, 3, padding=\"same\")\n",
        "    self.mxpl2 = nn.MaxPool2d(2)\n",
        "    self.lif2 = snn.Leaky(beta=beta)\n",
        "\n",
        "    self.fc = nn.Linear(128 * 7 * 7, num_outputs)\n",
        "    self.lif3 = snn.Leaky(beta=beta)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    # initialize hidden states at t=0\n",
        "    mem1 = self.lif1.init_leaky()\n",
        "    mem2 = self.lif2.init_leaky()\n",
        "    mem3 = self.lif3.init_leaky()\n",
        "\n",
        "    # Record the final layer\n",
        "    spk3_rec = []\n",
        "    mem3_rec = []\n",
        "\n",
        "    # time-loop\n",
        "    for step in range(num_steps):\n",
        "      cur1 = self.conv1(x)\n",
        "      spk1, mem1 = self.lif1(self.mxpl1(cur1), mem1)\n",
        "      cur2 = self.conv2(spk1)\n",
        "      spk2, mem2 = self.lif2(self.mxpl2(cur2), mem2)\n",
        "      cur3 = self.fc(spk2.flatten(1))\n",
        "      spk3, mem3 = self.lif2(cur3, mem3)\n",
        "\n",
        "      # store in list\n",
        "      spk3_rec.append(spk3)\n",
        "      mem3_rec.append(mem3)\n",
        "\n",
        "    return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0)"
      ],
      "metadata": {
        "id": "eRcVHADbbiHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one of many / more convoluted xixi\n",
        "class ConvNet2(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # initialize layers\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3, padding=\"same\")\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.mxpl1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(64, 128, 3, padding=\"same\")\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.mxpl2 = nn.MaxPool2d(2)\n",
        "        self.conv3 = nn.Conv2d(128, 256, 3, padding=\"same\")\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.mxpl3 = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(256 * 3 * 3, 512)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(512, num_outputs)\n",
        "\n",
        "        # initialize leaky integrate-and-fire activation function\n",
        "        self.lif1 = snn.Leaky(beta=beta)\n",
        "        self.lif2 = snn.Leaky(beta=beta)\n",
        "        self.lif3 = snn.Leaky(beta=beta)\n",
        "        self.lif4 = snn.Leaky(beta=beta)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # initialize hidden state at t=0\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        mem3 = self.lif3.init_leaky()\n",
        "        mem4 = self.lif4.init_leaky()\n",
        "\n",
        "        # Record the final layer\n",
        "        spk4_rec = []\n",
        "        mem4_rec = []\n",
        "\n",
        "        # time-loop\n",
        "        for step in range(num_steps):\n",
        "            cur1 = self.conv1(x)\n",
        "            cur1 = self.relu1(cur1)\n",
        "            spk1, mem1 = self.lif1(self.mxpl1(cur1), mem1)\n",
        "            cur2 = self.conv2(spk1)\n",
        "            cur2 = self.relu2(cur2)\n",
        "            spk2, mem2 = self.lif2(self.mxpl2(cur2), mem2)\n",
        "            cur3 = self.conv3(spk2)\n",
        "            cur3 = self.relu3(cur3)\n",
        "            spk3, mem3 = self.lif3(self.mxpl3(cur3), mem3)\n",
        "            cur4 = self.fc1(spk3.flatten(1))\n",
        "            cur4 = self.relu4(cur4)\n",
        "            spk4, mem4 = self.lif4(self.fc2(cur4), mem4)\n",
        "\n",
        "            # store in list\n",
        "            spk4_rec.append(spk4)\n",
        "            mem4_rec.append(mem4)\n",
        "\n",
        "        return torch.stack(spk4_rec, dim=0), torch.stack(mem4_rec, dim=0)"
      ],
      "metadata": {
        "id": "Pl9s0Br2ylVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the network to device\n",
        "#net = Net().to(device)\n",
        "net = ConvNet2().to(device)\n",
        "\n",
        "# Training Parameters\n",
        "num_epochs = 10\n",
        "counter = 0\n",
        "loss_hist = []\n",
        "test_loss_hist = []\n",
        "\n",
        "# Define Loss and Optimizer\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(cnvnet.parameters(), lr=1e-3, betas=(0.9, 0.999))"
      ],
      "metadata": {
        "id": "MQqddXnyKQjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# outer training loop -- SPIKE TRAINING LOOP\n",
        "for epoch in range(num_epochs):\n",
        "  train_batch = iter(train_loader)\n",
        "\n",
        "\n",
        "  # Minibatch training loop\n",
        "  for data, targets in train_batch:\n",
        "    data = data.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    # forward pass\n",
        "    net.train()\n",
        "    spk_rec, _ = net(data)\n",
        "\n",
        "    # initialize the loss & sum over time\n",
        "    loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "    loss_val = loss(spk_rec.sum(0), targets) # batch x num_out\n",
        "\n",
        "    # Gradient calculation & weight update\n",
        "    optimizer.zero_grad()\n",
        "    loss_val.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Store loss history for future plotting\n",
        "    #loss_hist.append(loss_val.item()) ---> forget in colab\n",
        "\n",
        "    # Print train/test loss/accuracy\n",
        "    if counter % 10 == 0:\n",
        "      print(f\"Iteration: {counter} \\t Train Loss: {loss_val.item()}\")\n",
        "    \n",
        "    counter += 1\n",
        "    if counter == 100:\n",
        "      break"
      ],
      "metadata": {
        "id": "oOZt75yyLu_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure Accuracy\n",
        "\n",
        "def measure_accuracy(model, dataloader):\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    running_length = 0\n",
        "    running_accuracy = 0\n",
        "\n",
        "    for data, targets in iter(dataloader):\n",
        "      print(type(targets))\n",
        "\n",
        "      data = data.to(device)\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      #forward pass\n",
        "      spk_rec, _ = model(data)\n",
        "      spike_count = spk_rec.sum(0)\n",
        "      _, max_spike = spike_count.max(1)\n",
        "\n",
        "      #correct classes for one batch\n",
        "      num_correct = (max_spike == targets).sum()\n",
        "\n",
        "      #total accuracy\n",
        "      running_length += len(targets)\n",
        "      running_accuracy += num_correct\n",
        "\n",
        "    accuracy = (running_accuracy / running_length)\n",
        "    return accuracy.item()"
      ],
      "metadata": {
        "id": "idYb5LAHQmmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Test set accuracy: {measure_accuracy(net, val_loader)}\")"
      ],
      "metadata": {
        "id": "ZWcgDDgWSe6q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}